{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "next Text Prediction in the book.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "nN2I28snHPBN",
        "outputId": "932b8ed3-37ce-47a7-8295-1f35dc9c5c0d"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "\n",
        "\n",
        "filename = \"the jungle.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "print(raw_text[0:1000])\n",
        "\n",
        "\n",
        "raw_text = ''.join(c for c in raw_text if not c.isdigit())\n",
        "\n",
        "\n",
        "chars = sorted(list(set(raw_text))) #List of every character\n",
        "\n",
        "\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# summarize the data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters in the text; corpus length: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "seq_length = 60  \n",
        "step = 10   \n",
        "sentences = []    \n",
        "next_chars = []   \n",
        "for i in range(0, n_chars - seq_length, step):  \n",
        "    sentences.append(raw_text[i: i + seq_length])  \n",
        "    next_chars.append(raw_text[i + seq_length])  \n",
        "n_patterns = len(sentences)    \n",
        "print('Number of sequences:', n_patterns)\n",
        "\n",
        "\n",
        "\n",
        "x = np.zeros((len(sentences), seq_length, n_vocab), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), n_vocab), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_to_int[char]] = 1\n",
        "    y[i, char_to_int[next_chars[i]]] = 1\n",
        "    \n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "\n",
        "print(y[0:10])\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(seq_length, n_vocab)))\n",
        "model.add(Dense(n_vocab, activation='softmax'))\n",
        "\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# define the checkpoint\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"saved_weights/saved_weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "# Fit the model\n",
        "\n",
        "history = model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=50,   \n",
        "          callbacks=callbacks_list)\n",
        "\n",
        "model.save('my_saved_weights_jungle_book_50epochs.h5')\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "#plot the training and validation accuracy and loss at each epoch\n",
        "loss = history.history['loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.title('Training loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "####################################################################\n",
        "###################################\n",
        "#Generate characters \n",
        "#We must provide a sequence of seq_lenth as input to start the generation process\n",
        "\n",
        "#The prediction results is probabilities for each of the 48 characters at a specific\n",
        "#point in sequence. Let us pick the one with max probability and print it out.\n",
        "#Writing our own softmax function....\n",
        "\n",
        "def sample(preds):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds)\n",
        "    exp_preds = np.exp(preds) #exp of log (x), isn't this same as x??\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1) \n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "\n",
        "#Prediction\n",
        "# load the network weights\n",
        "filename = \"my_saved_weights_jungle_book_50epochs.h5\"\n",
        "model.load_weights(filename)\n",
        "\n",
        "#Pick a random sentence from the text as seed.\n",
        "start_index = random.randint(0, n_chars - seq_length - 1)\n",
        "\n",
        "#Initiate generated text and keep adding new predictions and print them out\n",
        "generated = ''\n",
        "sentence = raw_text[start_index: start_index + seq_length]\n",
        "generated += sentence\n",
        "\n",
        "print('----- Seed for our text prediction: \"' + sentence + '\"')\n",
        "#sys.stdout.write(generated)\n",
        "\n",
        "\n",
        "for i in range(400):   # Number of characters including spaces\n",
        "    x_pred = np.zeros((1, seq_length, n_vocab))\n",
        "    for t, char in enumerate(sentence):\n",
        "        x_pred[0, t, char_to_int[char]] = 1.\n",
        "\n",
        "    preds = model.predict(x_pred, verbose=0)[0]\n",
        "    next_index = sample(preds)\n",
        "    next_char = int_to_char[next_index]\n",
        "\n",
        "    generated += next_char\n",
        "    sentence = sentence[1:] + next_char\n",
        "\n",
        "    sys.stdout.write(next_char)\n",
        "    sys.stdout.flush()\n",
        "print()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-04b443376fb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'RMSprop' from 'keras.optimizers' (/usr/local/lib/python3.7/dist-packages/keras/optimizers.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}